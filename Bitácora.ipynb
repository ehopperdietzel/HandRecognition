{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitácora Creación CNN Géstos de Mano\n",
    "El objetivo de este Notebook es registrar y compartir las experiencias adquiridas al desarrollar una CNN destinada al reconicimiento de gestos de mano.\n",
    "\n",
    "* Curso: INFO257 Inteligencia Artificial\n",
    "* Docente: Pablo Huijse\n",
    "\n",
    "#### Integrantes\n",
    "* Eduardo Hopperdietzel\n",
    "* Sebastián Lara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de Nuestro Modelo\n",
    "\n",
    "Tal como nos recomendó el profesor, comenzamos imitando la estructura del modelo Lenet5, luego fuimos quitando y añadiendo neuronas/capas, y jugando con los tamaños de kernel, stride y cantidad de padding.\n",
    "\n",
    "Pronto nos dimos cuenta que aumentar el número de neuronas mejoraba considerablemente la precisión del modelo, no así la cantidad de capas.\n",
    "\n",
    "### Neuronas\n",
    "\n",
    "Comenzamos a decidir el número de neuronas teniendo en cuenta el tamaño de las imágenes, la cantidad canales por pixel, cantidad de clases, cantidad de patrones o formas de los dedos, etc, y decidimos por ejemplo que la salida de una capa sea el triple de la entrada anterior (para que pueda aprender patrones por cada color RGB). Luego simplemente fuimos probando con la cantidad de neuronas por capa, partiendo inicialmente con 4, luego 8, 16, finalizando con 32 neuronas, con la que obtuvimos mejores resultados.\n",
    "\n",
    "### Capas\n",
    "Partimos utilizando entre 1 y 3 capas con 16 neuronas cada una aprox, obteniendo resultados regualares, luego probamos con muchas capas 8 a 16 sin muchas neuronas y el resultado fue peor, por lo tanto volvimos a utilizar pocas con muchas neuronas (16 a 256), y fuimos añadiendo capas una a una hasta que obtuvimos los mejores resultados con 5.\n",
    "Luego fuimos reduciendo poco a poco la cantidad de neuronas hasta el punto en que notamos comenzó a empeorar nuevamente.\n",
    "\n",
    "### Kernel\n",
    "Probamos diversos tamaños de kernel por capa, y nos dimos cuenta que obteniamos mejores resultados comenzando con tamaños grandes en las primeras y luego pequeños para las últimas.\n",
    "Tambien notamos que al usar un kernel relativamente grande (11) en la primera capa, tras entrenar los modelos podíamos ver que se generaban patrones, por lo tanto lo mantuvimos.\n",
    "\n",
    "*Filtros de la Primera Capa*\n",
    "![patrones](./img/filtros.png)\n",
    "\n",
    "( Creemos que podríamos eliminar algunas neuronas dado que algunos filtros solo parecieran almacenar ruido )\n",
    "### Pooling y Stride\n",
    "Aplicamos MaxPooling y Stride mayores a las capas con el fin de simplificar el modelo, fuimos aumentando hasta que creemos no se vió afectado la precisión del modelo.\n",
    "También aplicamos AveragePooling en la última capa de convulsión para suavisar la salida a la capa fully connected.\n",
    "\n",
    "### Capa Fully Connected\n",
    "Decidimos mantener la cantidad de capas fully connected de Lenet5, ya que obtuvimos horrendos resultados con otras cantidades, sin embargo aumentar el número de neuronas resultó ser muy efectivo.\n",
    "\n",
    "### Función de Activación\n",
    "\n",
    "Probamos la función ReLU y Tanh, y no notamos mucha diferencia, pero finalmente optamos por ReLU ya que en varios sitios web la recomendaban para CNNs.\n",
    "\n",
    "### Tamaño del Batch\n",
    "Probamos distintos tamaños, desde 1 hasta 200, y observamos que con tamaños pequeños la loss de validación nunca bajaba ni el acurracy subia.\n",
    "Con tamaños un poco más grandes 16-32 la loss y acurracy de entrenamiento y validación, presentaba una mejora coherente en cada época.\n",
    "\n",
    "![patrones](./img/tensor.png)\n",
    "\n",
    "### Checkpoints\n",
    "Inicialmente utilizamos la función de checkpoint de ignite, almacenando los modelos con menor loss de entrenamiento, pero tras visualizar las gráficas, nos surgieron dudas sobre si era la mejor métrica para tener en cuenta. Por lo tanto, decidimos almacenar los modelos de todas las épocas ( ya que con nuestras últimas estructuras pesaban aprox 5mb ), y luego probabamos cáda uno en el dataset de prueba y validación, midiendo su acurracy promedio.\n",
    "\n",
    "*( Casi siempre el mejor modelo de todas las épocas lo encontrabamos entre la 70 y 120 )*\n",
    "\n",
    "### Pre-Procesamiento de Imágenes\n",
    "Comenzamos a entrenar los modelos sin añadir transformaciones al dataset de entrenamiento, luego probamos reduciendo las resoluciones a 100px x 100px, 64px x 64px, etc y pudimos notar una leve mejora, sin embargo luego de observar las fotos del dataset de prueba, nos dimos cuenta que la mayoría de los brazos se encontraban inclinadas con un ángulo de aprox 20º, por lo tanto les aplicamos una rotación random de -20º a 20º, y un flip horizontal también random al dataset de entrenamiento y nos sorprendió la mejora.\n",
    "También intentamos reducir el bit depth de los colores (para acelerar el entrenamiento) pero no encontramos la manera.\n",
    "\n",
    "### Estructura Final\n",
    "Finalmente tras probar muchas combinaciones la mejor estructura que obtuvimos fue la siguiente:\n",
    "\n",
    "```python\n",
    "class HandModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(type(self), self).__init__()\n",
    "        self.c1 = nn.Conv2d(3, 32, kernel_size=11, stride=4, padding=2)\n",
    "        self.c2 = nn.Conv2d(32,96, kernel_size=5, padding=2)\n",
    "        self.c3 = nn.Conv2d(96,192, kernel_size=3, padding=1)\n",
    "        self.c4 = nn.Conv2d(192, 128, kernel_size=3, padding=1)\n",
    "        self.c5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.ac = nn.ReLU(inplace=True)\n",
    "        self.mp = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.ap = nn.AdaptiveAvgPool2d((3, 3))\n",
    "        self.f1 = nn.Linear(1152, 512)\n",
    "        self.f2 = nn.Linear(512, 512)\n",
    "        self.f3 = nn.Linear(512, 4)\n",
    "    def forward(self, x):\n",
    "        x = self.mp(self.ac(self.c1(x)))\n",
    "        x = self.mp(self.ac(self.c2(x)))\n",
    "        x = self.ac(self.c3(x))\n",
    "        x = self.ac(self.c4(x))\n",
    "        x = self.mp(self.ac(self.c5(x)))\n",
    "        x = self.ap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.ac(self.f1(x))\n",
    "        x = self.ac(self.f2(x))\n",
    "        x = self.f3(x)\n",
    "        return x\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendimiento\n",
    "\n",
    "## Matriz de Confusión\n",
    "\n",
    "![patrones](./img/ownCM.png)\n",
    "\n",
    "## Imágenes Mal Clasificadas\n",
    "\n",
    "![patrones](./img/ownErr.png)\n",
    "\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
